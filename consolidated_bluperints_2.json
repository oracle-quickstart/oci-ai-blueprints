[{"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Fine-Tuning Benchmarking", "blueprint_short_description": "Fine-tune quantized Llama-2-70B model using MLCommons methodology for infrastructure benchmarking", "blueprint_long_description": "The fine-tuning benchmarking blueprint streamlines infrastructure benchmarking for fine-tuning using the MLCommons methodology. It fine-tunes a quantized Llama-2-70B model and a standard dataset.\n\nOnce complete, benchmarking results, such as training time and resource utilization, are available in MLFlow and Grafana for easy tracking. This blueprint enables data-driven infrastructure decisions for your fine-tuning jobs.", "pre_filled_samples": [{"pre_filled_sample_name": "LoRA fine-tuning of quantitized Llama-2-70B model on A100 node using MLCommons methodology", "recipe_id": "mlcommons_lora_finetune_nvidia", "deployment_name": "MLCommons Finetune LORA/PEFT", "recipe_mode": "job", "recipe_node_shape": "BM.GPU.A100.8", "recipe_use_shared_node_pool": false, "recipe_nvidia_gpu_count": 8, "recipe_ephemeral_storage_size": 50, "recipe_replica_count": 1, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_shared_memory_volume_size_limit_in_mb": 100, "input_object_storage": [{"bucket_name": "corrino_mlcommons_llama2_70b_qkv", "mount_location": "/models", "volume_size_in_gbs": 500}, {"bucket_name": "corrino_ml_commons_scrolls_dataset", "mount_location": "/dataset", "volume_size_in_gbs": 100}], "output_object_storage": [{"bucket_name": "corrino_ml_commons_output", "mount_location": "/mlcommons_output", "volume_size_in_gbs": 200}], "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:corrino-recipe-mlcommons", "recipe_container_env": [{"key": "model_name", "value": "regisss/llama2-70b-fused-qkv-mlperf"}, {"key": "Model_Path", "value": "/models"}, {"key": "Dataset_Path", "value": "/dataset"}, {"key": "Lora_R", "value": "16"}, {"key": "Lora_Alpha", "value": "32"}, {"key": "Lora_Dropout", "value": "0.1"}, {"key": "Max_Seq_Len", "value": "8192"}, {"key": "bf16", "value": "true"}, {"key": "Logging_Steps", "value": "24"}, {"key": "Eval_Steps", "value": "48"}, {"key": "Per_Device_Train_Batch_Size", "value": "1"}, {"key": "Gradient_Accumulation_Steps", "value": "1"}, {"key": "Lr_Scheduler_Type", "value": "cosine"}, {"key": "Learning_Rate", "value": "0.0004"}, {"key": "Weight_Decay", "value": "0.0001"}, {"key": "Warmup_Ratio", "value": "0"}, {"key": "Max_Grad_Norm", "value": "0.3"}, {"key": "Use_Gradient_Checkpointing", "value": "true"}, {"key": "Target_Eval_Loss", "value": "0.925"}, {"key": "Use_Peft_Lora", "value": "true"}, {"key": "Max_Steps", "value": "1024"}, {"key": "Use_Flash_Attn", "value": "true"}, {"key": "Seed", "value": "1234"}, {"key": "Lora_Target_Modules", "value": "qkv_proj,o_proj"}, {"key": "Mlflow_Exp_Name", "value": "oci_ai_blueprints_nvidia_recipe"}, {"key": "Output_Dir", "value": "/mlcommons_output"}]}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "GPU Health Check", "blueprint_short_description": "Comprehensive GPU health validation and diagnostics for production readiness", "blueprint_long_description": "This repository offers a robust, pre-check recipe for thorough GPU health validation prior to deploying production or research workloads. Designed to operate seamlessly across both single-node and multi-node environments, this diagnostic toolset enables you to verify that your GPU infrastructure is primed for high-demand experiments. By systematically assessing key performance metrics—such as thermal behavior, power stability, and overall hardware reliability—you can proactively detect and address issues like thermal throttling, power irregularities, and GPU instability. This early-warning system minimizes the risk of unexpected downtime and performance degradation, ensuring that your system consistently operates at peak efficiency and reliability during critical computational tasks.", "pre_filled_samples": [{"pre_filled_sample_name": "2 A10 GPUs with dtype 16", "recipe_id": "healthcheck", "recipe_mode": "job", "deployment_name": "healthcheck_fp16_a10", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:healthcheck_v0.3", "recipe_node_shape": "VM.GPU.A10.2", "output_object_storage": [{"bucket_name": "healthcheck2", "mount_location": "/healthcheck_results", "volume_size_in_gbs": 20}], "recipe_container_command_args": ["--dtype", "float16", "--output_dir", "/healthcheck_results", "--expected_gpus", "A10:2,A100:0,H100:0"], "recipe_replica_count": 1, "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}, {"pre_filled_sample_name": "2 A10 GPUs with dtype 32", "recipe_id": "healthcheck", "recipe_mode": "job", "deployment_name": "healthcheck_fp32_a10", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:healthcheck_v0.3", "recipe_node_shape": "VM.GPU.A10.2", "output_object_storage": [{"bucket_name": "healthcheck2", "mount_location": "/healthcheck_results", "volume_size_in_gbs": 20}], "recipe_container_command_args": ["--dtype", "float32", "--output_dir", "/healthcheck_results", "--expected_gpus", "A10:2,A100:0,H100:0"], "recipe_replica_count": 1, "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}, {"pre_filled_sample_name": "8 H100 GPUs with dtype 16", "recipe_id": "healthcheck", "recipe_mode": "job", "deployment_name": "healthcheck_fp16_h100", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:healthcheck_v0.3", "recipe_node_shape": "BM.GPU.H100.8", "output_object_storage": [{"bucket_name": "healthcheck2", "mount_location": "/healthcheck_results", "volume_size_in_gbs": 20}], "recipe_container_command_args": ["--dtype", "float16", "--output_dir", "/healthcheck_results", "--expected_gpus", "A10:0,A100:0,H100:8"], "recipe_replica_count": 1, "recipe_nvidia_gpu_count": 8, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "LoRA Fine-Tuning", "blueprint_short_description": "Efficiently fine-tune large language models using Low-Rank Adaptation", "blueprint_long_description": "This blueprint enables efficient model tuning using Low-Rank Adaptation (LoRA), a highly efficient method of LLM tuning. You can fine-tune a custom LLM or most open-source LLMs from Hugging Face. You can also use a custom dataset or any publicly available dataset from Hugging Face. Once the job is complete, results such as training metrics and logged in MLFlow for analysis. The fine-tuned model is then stored in an object storage bucket, ready for deployment.", "pre_filled_samples": [{"pre_filled_sample_name": "Fine-Tune NousResearch/Meta-Llama-3.1-8B from Object Storage with Dataset from Hugging Face and Checkpoints saved in Object Storage (A10 VM)", "recipe_id": "lora_finetune_nvidia", "deployment_name": "dk_with_checkpoint", "recipe_mode": "job", "recipe_node_shape": "VM.GPU.A10.2", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:finetune_lora_dev", "recipe_nvidia_gpu_count": 2, "recipe_ephemeral_storage_size": 300, "recipe_replica_count": 1, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 500, "recipe_shared_memory_volume_size_limit_in_mb": 100, "recipe_container_env": [{"key": "Mlflow_Endpoint", "value": "http://mlflow.cluster-tools.svc.cluster.local:5000"}, {"key": "Mlflow_Exp_Name", "value": "oci_ai_blueprints_nvidia_recipe"}, {"key": "Mlflow_Run_Name", "value": "Meta-Llama-3.1-8B-local-quotes-checkpoint"}, {"key": "Hf_Token", "value": "None"}, {"key": "Download_Dataset_From_Hf", "value": "true"}, {"key": "Dataset_Name", "value": "Abirate/english_quotes"}, {"key": "Dataset_Sub_Name", "value": "None"}, {"key": "Dataset_Column_To_Use", "value": "quote"}, {"key": "Dataset_Path", "value": "/workspace/datasets"}, {"key": "Download_Model_From_Hf", "value": "false"}, {"key": "Model_Name", "value": "NousResearch/Meta-Llama-3.1-8B"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B"}, {"key": "Max_Model_Length", "value": "4096"}, {"key": "Resume_From_Checkpoint", "value": "true"}, {"key": "Checkpoint_Path", "value": "/checkpoint/Bucket-Llama-3.1-8B-english_quotes/checkpoint-1400"}, {"key": "Lora_R", "value": "8"}, {"key": "Lora_Alpha", "value": "16"}, {"key": "Lora_Dropout", "value": "0.1"}, {"key": "Lora_Target_Modules", "value": "q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj"}, {"key": "Bias", "value": "none"}, {"key": "Task_Type", "value": "CAUSAL_LM"}, {"key": "Per_Device_Train_Batch_Size", "value": "1"}, {"key": "Gradient_Accumulation_Steps", "value": "1"}, {"key": "Warmup_Steps", "value": "2"}, {"key": "Save_Steps", "value": "100"}, {"key": "Learning_Rate", "value": "0.0002"}, {"key": "Fp16", "value": "true"}, {"key": "Logging_Steps", "value": "1"}, {"key": "Output_Dir", "value": "/tunedmodels/Bucket-Llama-3.1-8B-english_quotes"}, {"key": "Optim", "value": "paged_adamw_8bit"}, {"key": "Number_of_Training_Epochs", "value": "2"}, {"key": "Require_Persistent_Output_Dir", "value": "true"}], "input_object_storage": [{"bucket_name": "corrino_hf_oss_models", "mount_location": "/models", "volume_size_in_gbs": 500}, {"bucket_name": "corrino_tuned_hf_oss_models", "mount_location": "/checkpoint", "volume_size_in_gbs": 500}], "output_object_storage": [{"bucket_name": "corrino_tuned_hf_oss_models", "mount_location": "/tunedmodels", "volume_size_in_gbs": 500}]}, {"pre_filled_sample_name": "Fine-Tune NousResearch/Meta-Llama-3.1-8B from Object Storage with Dataset from Hugging Face on A10 VM", "recipe_id": "lora_finetune_nvidia", "deployment_name": "dk_bucket_model_open_dataset", "recipe_mode": "job", "recipe_node_shape": "VM.GPU.A10.2", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:finetune_lora_dev", "recipe_nvidia_gpu_count": 2, "recipe_ephemeral_storage_size": 300, "recipe_replica_count": 1, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 500, "recipe_shared_memory_volume_size_limit_in_mb": 100, "recipe_container_env": [{"key": "Mlflow_Endpoint", "value": "http://mlflow.cluster-tools.svc.cluster.local:5000"}, {"key": "Mlflow_Exp_Name", "value": "oci_ai_blueprints_nvidia_recipe"}, {"key": "Mlflow_Run_Name", "value": "Meta-Llama-3.1-8B-local-quotes"}, {"key": "Hf_Token", "value": "None"}, {"key": "Download_Dataset_From_Hf", "value": "true"}, {"key": "Dataset_Name", "value": "Abirate/english_quotes"}, {"key": "Dataset_Sub_Name", "value": "None"}, {"key": "Dataset_Column_To_Use", "value": "None"}, {"key": "Dataset_Path", "value": "/workspace/datasets"}, {"key": "Download_Model_From_Hf", "value": "false"}, {"key": "Model_Name", "value": "NousResearch/Meta-Llama-3.1-8B"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B"}, {"key": "Max_Model_Length", "value": "8192"}, {"key": "Resume_From_Checkpoint", "value": "false"}, {"key": "Checkpoint_Path", "value": "/checkpoint"}, {"key": "Lora_R", "value": "8"}, {"key": "Lora_Alpha", "value": "32"}, {"key": "Lora_Dropout", "value": "0.1"}, {"key": "Lora_Target_Modules", "value": "q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj"}, {"key": "Bias", "value": "none"}, {"key": "Task_Type", "value": "CAUSAL_LM"}, {"key": "Per_Device_Train_Batch_Size", "value": "1"}, {"key": "Gradient_Accumulation_Steps", "value": "1"}, {"key": "Warmup_Steps", "value": "2"}, {"key": "Save_Steps", "value": "100"}, {"key": "Learning_Rate", "value": "0.0002"}, {"key": "Fp16", "value": "true"}, {"key": "Logging_Steps", "value": "1"}, {"key": "Output_Dir", "value": "/tunedmodels/Bucket-Llama-3.1-8B-english_quotes"}, {"key": "Optim", "value": "paged_adamw_8bit"}, {"key": "Number_of_Training_Epochs", "value": "2"}, {"key": "Require_Persistent_Output_Dir", "value": "true"}], "input_object_storage": [{"bucket_name": "corrino_hf_oss_models", "mount_location": "/models", "volume_size_in_gbs": 500}], "output_object_storage": [{"bucket_name": "corrino_tuned_hf_oss_models", "mount_location": "/tunedmodels", "volume_size_in_gbs": 500}]}, {"pre_filled_sample_name": "Fine-Tune NousResearch/Meta-Llama-3.1-8B from Object Storage (PAR link) with Dataset from Hugging Face on A10 VM", "recipe_id": "lora_finetune_nvidia", "deployment_name": "dk_bucket_model_open_dataset", "recipe_mode": "job", "recipe_node_shape": "VM.GPU.A10.2", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:finetune_lora_dev", "recipe_nvidia_gpu_count": 2, "recipe_ephemeral_storage_size": 300, "recipe_replica_count": 1, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 500, "recipe_shared_memory_volume_size_limit_in_mb": 100, "recipe_container_env": [{"key": "Mlflow_Endpoint", "value": "http://mlflow.cluster-tools.svc.cluster.local:5000"}, {"key": "Mlflow_Exp_Name", "value": "oci_ai_blueprints_nvidia_recipe"}, {"key": "Mlflow_Run_Name", "value": "Meta-Llama-3.1-8B-local-quotes"}, {"key": "Hf_Token", "value": "None"}, {"key": "Download_Dataset_From_Hf", "value": "true"}, {"key": "Dataset_Name", "value": "Abirate/english_quotes"}, {"key": "Dataset_Sub_Name", "value": "None"}, {"key": "Dataset_Column_To_Use", "value": "None"}, {"key": "Dataset_Path", "value": "/workspace/datasets"}, {"key": "Download_Model_From_Hf", "value": "false"}, {"key": "Model_Name", "value": "NousResearch/Meta-Llama-3.1-8B"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B"}, {"key": "Max_Model_Length", "value": "8192"}, {"key": "Resume_From_Checkpoint", "value": "false"}, {"key": "Checkpoint_Path", "value": "/checkpoint"}, {"key": "Lora_R", "value": "8"}, {"key": "Lora_Alpha", "value": "32"}, {"key": "Lora_Dropout", "value": "0.1"}, {"key": "Lora_Target_Modules", "value": "q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj"}, {"key": "Bias", "value": "none"}, {"key": "Task_Type", "value": "CAUSAL_LM"}, {"key": "Per_Device_Train_Batch_Size", "value": "1"}, {"key": "Gradient_Accumulation_Steps", "value": "1"}, {"key": "Warmup_Steps", "value": "2"}, {"key": "Save_Steps", "value": "100"}, {"key": "Learning_Rate", "value": "0.0002"}, {"key": "Fp16", "value": "true"}, {"key": "Logging_Steps", "value": "1"}, {"key": "Output_Dir", "value": "/tunedmodels/Bucket-Llama-3.1-8B-english_quotes"}, {"key": "Optim", "value": "paged_adamw_8bit"}, {"key": "Number_of_Training_Epochs", "value": "2"}, {"key": "Require_Persistent_Output_Dir", "value": "true"}], "input_object_storage": [{"par": "https://objectstorage.us-phoenix-1.oraclecloud.com/p/iv-8F3oSRJ8nsbVaq9ev9kjfkZ3zXItSOCSDWKfRa7zT3aPmNf4MijL_4nw_hvvY/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B"]}], "output_object_storage": [{"bucket_name": "corrino_tuned_hf_oss_models", "mount_location": "/tunedmodels", "volume_size_in_gbs": 500}]}, {"pre_filled_sample_name": "Fine-Tune meta-llama/Llama-3.2-1B-Instruct (Closed Model) from Hugging Face with Dataset from Hugging Face on A10 VM", "recipe_id": "lora_finetune_nvidia", "deployment_name": "dk_closed_model_open_dataset", "recipe_mode": "job", "recipe_node_shape": "VM.GPU.A10.2", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:finetune_lora_dev", "recipe_nvidia_gpu_count": 2, "recipe_ephemeral_storage_size": 300, "recipe_replica_count": 1, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 500, "recipe_shared_memory_volume_size_limit_in_mb": 100, "recipe_container_env": [{"key": "Mlflow_Endpoint", "value": "http://mlflow.cluster-tools.svc.cluster.local:5000"}, {"key": "Mlflow_Exp_Name", "value": "oci_ai_blueprints_nvidia_recipe"}, {"key": "Mlflow_Run_Name", "value": "llama-3.2-1B-Instruct-scrolls-gov_report"}, {"key": "Hf_Token", "value": "<hf_token>"}, {"key": "Download_Dataset_From_Hf", "value": "true"}, {"key": "Dataset_Name", "value": "tau/scrolls"}, {"key": "Dataset_Sub_Name", "value": "gov_report"}, {"key": "Dataset_Column_To_Use", "value": "None"}, {"key": "Dataset_Path", "value": "/workspace/datasets"}, {"key": "Download_Model_From_Hf", "value": "true"}, {"key": "Model_Name", "value": "meta-llama/Llama-3.2-1B-Instruct"}, {"key": "Model_Path", "value": "/workspace/models"}, {"key": "Max_Model_Length", "value": "8192"}, {"key": "Resume_From_Checkpoint", "value": "false"}, {"key": "Checkpoint_Path", "value": "/checkpoint"}, {"key": "Lora_R", "value": "8"}, {"key": "Lora_Alpha", "value": "32"}, {"key": "Lora_Dropout", "value": "0.1"}, {"key": "Lora_Target_Modules", "value": "q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj"}, {"key": "Bias", "value": "none"}, {"key": "Task_Type", "value": "CAUSAL_LM"}, {"key": "Per_Device_Train_Batch_Size", "value": "1"}, {"key": "Gradient_Accumulation_Steps", "value": "1"}, {"key": "Warmup_Steps", "value": "2"}, {"key": "Save_Steps", "value": "100"}, {"key": "Learning_Rate", "value": "0.0002"}, {"key": "Fp16", "value": "true"}, {"key": "Logging_Steps", "value": "1"}, {"key": "Output_Dir", "value": "/tunedmodels/Llama-3.1-8B-english_quotes"}, {"key": "Optim", "value": "paged_adamw_8bit"}, {"key": "Number_of_Training_Epochs", "value": "2"}, {"key": "Require_Persistent_Output_Dir", "value": "true"}], "output_object_storage": [{"bucket_name": "corrino_tuned_hf_oss_models", "mount_location": "/tunedmodels", "volume_size_in_gbs": 500}]}, {"pre_filled_sample_name": "Fine-Tune NousResearch/Meta-Llama-3.1-8B (Open Model) from Hugging Face with Dataset from Hugging Face on A10 VM", "recipe_id": "lora_finetune_nvidia", "deployment_name": "dk_open_model_open_dataset", "recipe_mode": "job", "recipe_node_shape": "VM.GPU.A10.2", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:finetune_lora_dev", "recipe_nvidia_gpu_count": 2, "recipe_ephemeral_storage_size": 300, "recipe_replica_count": 1, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 500, "recipe_shared_memory_volume_size_limit_in_mb": 100, "recipe_container_env": [{"key": "Mlflow_Endpoint", "value": "http://mlflow.cluster-tools.svc.cluster.local:5000"}, {"key": "Mlflow_Exp_Name", "value": "oci_ai_blueprints_nvidia_recipe"}, {"key": "Mlflow_Run_Name", "value": "oci_ai_blueprints_run"}, {"key": "Hf_Token", "value": "None"}, {"key": "Download_Dataset_From_Hf", "value": "true"}, {"key": "Dataset_Name", "value": "Abirate/english_quotes"}, {"key": "Dataset_Sub_Name", "value": "None"}, {"key": "Dataset_Column_To_Use", "value": "None"}, {"key": "Dataset_Path", "value": "/workspace/datasets"}, {"key": "Download_Model_From_Hf", "value": "true"}, {"key": "Model_Name", "value": "NousResearch/Meta-Llama-3.1-8B"}, {"key": "Model_Path", "value": "/workspace/models"}, {"key": "Max_Model_Length", "value": "8192"}, {"key": "Resume_From_Checkpoint", "value": "false"}, {"key": "Checkpoint_Path", "value": "/checkpoint"}, {"key": "Lora_R", "value": "8"}, {"key": "Lora_Alpha", "value": "32"}, {"key": "Lora_Dropout", "value": "0.1"}, {"key": "Lora_Target_Modules", "value": "q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj"}, {"key": "Bias", "value": "none"}, {"key": "Task_Type", "value": "CAUSAL_LM"}, {"key": "Per_Device_Train_Batch_Size", "value": "1"}, {"key": "Gradient_Accumulation_Steps", "value": "1"}, {"key": "Warmup_Steps", "value": "2"}, {"key": "Save_Steps", "value": "100"}, {"key": "Learning_Rate", "value": "0.0002"}, {"key": "Fp16", "value": "true"}, {"key": "Logging_Steps", "value": "1"}, {"key": "Output_Dir", "value": "/tunedmodels/Llama-3.1-8B-english_quotes"}, {"key": "Optim", "value": "paged_adamw_8bit"}, {"key": "Number_of_Training_Epochs", "value": "2"}, {"key": "Require_Persistent_Output_Dir", "value": "true"}], "output_object_storage": [{"bucket_name": "corrino_tuned_hf_oss_models", "mount_location": "/tunedmodels", "volume_size_in_gbs": 500}]}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Autoscaling", "blueprint_short_description": "Scale inference workloads based on traffic load", "blueprint_long_description": "OCI AI Blueprints supports automatic scaling (autoscaling) of inference workloads to handle varying traffic loads efficiently. This means that when demand increases, OCI AI Blueprints can spin up more pods (containers running your inference jobs) and, if needed, provision additional GPU nodes. When demand decreases, it scales back down to save resources and cost.", "pre_filled_samples": [{"pre_filled_sample_name": "vLLM Inference with Automatic Scaling on VM.GPU.A10.2", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "autoscale_vllm_example", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "VM.GPU.A10.2", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/qFv5XzocpOoEXjlxL7Q3ZrrCFkx9GkA1fpg97zmnaNEX9WB_WMXLz2rykGuU1hqQ/n/iduyx1qnmway/b/metallama321binstruct/o/", "mount_location": "/models", "volume_size_in_gbs": 100}], "recipe_container_env": [{"key": "tensor_parallel_size", "value": "1"}, {"key": "model_name", "value": ""}, {"key": "Model_Path", "value": "/models"}], "recipe_prometheus_enabled": true, "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 1, "recipe_container_command_args": ["--model", "$(Model_Path)", "--tensor-parallel-size", "$(tensor_parallel_size)", "--gpu-memory-utilization", "0.95", "--max-model-len", "1024"], "recipe_ephemeral_storage_size": 200, "recipe_node_boot_volume_size_in_gbs": 300, "recipe_node_pool_size": 1, "recipe_shared_memory_volume_size_limit_in_mb": 200, "recipe_startup_probe_params": {"failure_threshold": 30, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 60, "period_seconds": 2, "success_threshold": 1, "timeout_seconds": 10}, "recipe_liveness_probe_params": {"failure_threshold": 3, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 65, "period_seconds": 600, "success_threshold": 1, "timeout_seconds": 10}, "recipe_node_autoscaling_params": {"min_nodes": 1, "max_nodes": 2}, "recipe_pod_autoscaling_params": {"min_replicas": 1, "max_replicas": 4}}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "CPU Inference", "blueprint_short_description": "Deploy CPU-based inference with Ollama for cost-effective and GPU-free model serving", "blueprint_long_description": "This blueprint provides a comprehensive framework for testing inference on CPUs using the Ollama platform with a variety of supported models such as Mistral, Gemma, and others available through Ollama. Unlike GPU-dependent solutions, this blueprint is designed for environments where CPU inference is preferred or required. It offers clear guidelines and configuration settings to deploy a robust CPU inference service, enabling thorough performance evaluations and reliability testing. Ollama's lightweight and efficient architecture makes it an ideal solution for developers looking to benchmark and optimize CPU-based inference workloads.\n\nThis blueprint explains how to use CPU inference for running large language models using Ollama. It includes two main deployment strategies:\n\n- Serving pre-saved models directly from Object Storage\n\n- Pulling models from Ollama and saving them to Object Storage", "pre_filled_samples": [{"pre_filled_sample_name": "CPU inference with Mistral and BM.Standard.E4", "recipe_id": "cpu_inference", "recipe_mode": "service", "deployment_name": "cpu Inference mistral BME4", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:cpu_inference_service_v0.2", "recipe_node_shape": "BM.Standard.E4.128", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/0LYMMBRGg_OEm_hzp9BG8BlQx7Ccpy3gY-gRzjQQFZRU6peG0pXyHTRHUGZLp82E/n/iduyx1qnmway/b/ollama-models/o/", "mount_location": "/models", "volume_size_in_gbs": 20}], "recipe_container_env": [{"key": "MODEL_NAME", "value": "mistral"}, {"key": "PROMPT", "value": "What is the capital of France?"}], "recipe_replica_count": 1, "recipe_container_port": "11434", "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--input_directory", "/models", "--model_name", "mistral"], "recipe_ephemeral_storage_size": 100}, {"pre_filled_sample_name": "CPU inference with Gemma and BM.Standard.E5.192", "recipe_id": "cpu_inference", "recipe_mode": "service", "deployment_name": "cpu Inference gemma BME5", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:cpu_inference_service_v0.2", "recipe_node_shape": "BM.Standard.E5.192", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/0LYMMBRGg_OEm_hzp9BG8BlQx7Ccpy3gY-gRzjQQFZRU6peG0pXyHTRHUGZLp82E/n/iduyx1qnmway/b/ollama-models/o/", "mount_location": "/models", "volume_size_in_gbs": 20}], "recipe_container_env": [{"key": "MODEL_NAME", "value": "gemma"}, {"key": "PROMPT", "value": "What is the capital of Germany?"}], "recipe_replica_count": 1, "recipe_container_port": "11434", "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--input_directory", "/models", "--model_name", "gemma"], "recipe_ephemeral_storage_size": 100}, {"pre_filled_sample_name": "CPU inference with mistral and VM.Standard.E4.Flex", "recipe_id": "cpu_inference", "recipe_mode": "service", "deployment_name": "cpu Inference mistral E4Flex", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:cpu_inference_service_v0.2", "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 4, "recipe_flex_shape_memory_size_in_gbs": 64, "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/0LYMMBRGg_OEm_hzp9BG8BlQx7Ccpy3gY-gRzjQQFZRU6peG0pXyHTRHUGZLp82E/n/iduyx1qnmway/b/ollama-models/o/", "mount_location": "/models", "volume_size_in_gbs": 20}], "recipe_container_env": [{"key": "MODEL_NAME", "value": "mistral"}, {"key": "PROMPT", "value": "What is the capital of Spain?"}], "recipe_replica_count": 1, "recipe_container_port": "11434", "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--input_directory", "/models", "--model_name", "mistral"], "recipe_ephemeral_storage_size": 100}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "LLM Inference with vLLM", "blueprint_short_description": "Deploy open-source LLMs to GPUs for inference with vLLM.", "blueprint_long_description": "This blueprint simplifies the deployment of LLMs using an open-source inference engine called vLLM. You can deploy a custom model or select from a variety of open-source models on Hugging Face.\n\nThe blueprint deploys the model from an object storage bucket to a GPU node in an OKE cluster in your tenancy. Once deployed, you receive a ready-to-use API endpoint to start generating responses from the model. For mission-critical workloads, you can also configure auto-scaling driven by application metrics like inference latency. To summarize, this blueprint streamlines inference deployment, making it easy to scale and integrate into your applications without deep, technical expertise.", "pre_filled_samples": [{"pre_filled_sample_name": "Meta-Llama-3.1-8B-Instruct from OCI Object Storage on VM.GPU.A10.2 with vLLM", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "vllm-model-from-obj-storage", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "VM.GPU.A10.2", "recipe_prometheus_enabled": true, "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]}], "recipe_container_env": [{"key": "tensor_parallel_size", "value": "2"}, {"key": "model_name", "value": "NousResearch/Meta-Llama-3.1-8B-Instruct"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B-Instruct"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "$(Model_Path)", "--tensor-parallel-size", "$(tensor_parallel_size)"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 200}, {"pre_filled_sample_name": "meta-llama/Llama-3.2-11B-Vision (Closed Model) from Hugging Face on VM.GPU.A10.2 with vLLM", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "vllm-closed-hf-model", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "VM.GPU.A10.2", "recipe_container_env": [{"key": "HF_TOKEN", "value": "<AUTH-TOKEN-HERE>"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_prometheus_enabled": true, "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "meta-llama/Llama-3.2-11B-Vision", "--tensor-parallel-size", "2"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 200}, {"pre_filled_sample_name": "NousResearch/Meta-Llama-3-8B-Instruct (Open Model) from Hugging Face on VM.GPU.A10.2 with vLLM", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "vllm-open-hf-model", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "VM.GPU.A10.2", "recipe_prometheus_enabled": true, "recipe_container_env": [{"key": "tensor_parallel_size", "value": "2"}, {"key": "model_name", "value": "NousResearch/Meta-Llama-3-8B-Instruct"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "$(model_name)", "--tensor-parallel-size", "$(tensor_parallel_size)"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 200}, {"pre_filled_sample_name": "NousResearch/Meta-Llama-3-8B-Instruct (Open Model) from Hugging Face on VM.GPU.A10.2 with vLLM and Endpoint API Key", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "vllm-open-hf-model-api-key-functionality", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "VM.GPU.A10.2", "recipe_prometheus_enabled": true, "recipe_container_env": [{"key": "VLLM_API_KEY", "value": "<ANY-API-KEY-HERE>"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "NousResearch/Meta-Llama-3-8B-Instruct", "--tensor-parallel-size", "2"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 200}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Multi-Instance GPU (MIG)", "blueprint_short_description": "Partition GPUs into multiple isolated instances for efficient resource sharing and concurrent workloads", "blueprint_long_description": "Multi-Instance GPU (MIG) is a feature of NVIDIA GPUs that allows a single physical GPU to be partitioned into multiple isolated instances, each acting as an independent GPU with dedicated compute, memory, and cache resources. This enables multiple users or workloads to run concurrently on a single GPU without interfering with each other and without virtualization overhead.\n\nMIG is particularly useful when running multiple smaller models that do not require an entire GPU, such as hosting multiple smaller LLMs (Llama-7B, Mistral-7B, or Gemma-2B) on an A100 or H100 GPU. It ensures resource allocation is optimized, preventing one model from monopolizing the entire GPU while maintaining high throughput. This approach is incredibly well-suited for autoscaling scenarios because many more pods can be scheduled onto a single node depending on the MIG configuration.\n\nCurrently, OCI AI Blueprints supports MIG for H100, H200, and B200s with various slice configurations ranging from 7 mini GPUs to full instances. The system supports creating MIG-enabled shared node pools, deploying inference workloads to specific MIG slices, and updating MIG configurations on existing nodes.\n\nTo see supported configurations and resource requests, go to [Mig Configurations](./README.md#mig-configurations).", "pre_filled_samples": [{"pre_filled_sample_name": "MIG-Enabled H100 Shared Node Pool", "deployment_name": "H100_pool_mig", "recipe_mode": "shared_node_pool", "shared_node_pool_size": 1, "shared_node_pool_shape": "BM.GPU.H100.8", "shared_node_pool_boot_volume_size_in_gbs": 1000, "shared_node_pool_mig_config": "all-1g.20gb"}, {"pre_filled_sample_name": "MIG Inference with Multiple Replicas and Autoscaling", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "autoscale_mig", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_prometheus_enabled": true, "recipe_node_shape": "BM.GPU.H100.8", "recipe_container_env": [{"key": "tensor_parallel_size", "value": "1"}, {"key": "HF_TOKEN", "value": "<hf_token>"}], "recipe_replica_count": 5, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 1, "recipe_use_shared_node_pool": true, "mig_resource_request": "1g.10gb", "recipe_container_command_args": ["--model", "meta-llama/Llama-3.2-3B-Instruct", "--dtype", "bfloat16", "--tensor-parallel-size", "$(tensor_parallel_size)", "--gpu-memory-utilization", "0.99", "--max-model-len", "16384"], "recipe_ephemeral_storage_size": 30, "recipe_node_boot_volume_size_in_gbs": 300, "recipe_shared_memory_volume_size_limit_in_mb": 1000, "recipe_startup_probe_params": {"failure_threshold": 30, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 10, "period_seconds": 2, "success_threshold": 1, "timeout_seconds": 1}, "recipe_liveness_probe_params": {"failure_threshold": 3, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 65, "period_seconds": 600, "success_threshold": 1, "timeout_seconds": 1}, "recipe_pod_autoscaling_params": {"min_replicas": 5, "max_replicas": 10}}, {"pre_filled_sample_name": "MIG Inference Single Replica (20GB Slice)", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "autoscale_mig", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "BM.GPU.H100.8", "recipe_prometheus_enabled": true, "recipe_container_env": [{"key": "tensor_parallel_size", "value": "1"}, {"key": "HF_TOKEN", "value": "<hf_token>"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 1, "recipe_use_shared_node_pool": true, "mig_resource_request": "1g.20gb", "recipe_container_command_args": ["--model", "meta-llama/Llama-3.2-3B-Instruct", "--dtype", "bfloat16", "--tensor-parallel-size", "$(tensor_parallel_size)", "--gpu-memory-utilization", "0.99", "--max-model-len", "16384"], "recipe_ephemeral_storage_size": 30, "recipe_node_boot_volume_size_in_gbs": 300, "recipe_shared_memory_volume_size_limit_in_mb": 1000, "recipe_startup_probe_params": {"failure_threshold": 30, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 10, "period_seconds": 2, "success_threshold": 1, "timeout_seconds": 1}, "recipe_liveness_probe_params": {"failure_threshold": 3, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 65, "period_seconds": 600, "success_threshold": 1, "timeout_seconds": 1}, "recipe_pod_autoscaling_params": {"min_replicas": 1, "max_replicas": 50}}, {"pre_filled_sample_name": "MIG Inference Single Replica (10GB Slice)", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "autoscale_mig", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "BM.GPU.H100.8", "recipe_prometheus_enabled": true, "recipe_container_env": [{"key": "tensor_parallel_size", "value": "1"}, {"key": "HF_TOKEN", "value": "<hf_token>"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 1, "recipe_use_shared_node_pool": true, "mig_resource_request": "1g.10gb", "recipe_container_command_args": ["--model", "meta-llama/Llama-3.2-3B-Instruct", "--dtype", "bfloat16", "--tensor-parallel-size", "$(tensor_parallel_size)", "--gpu-memory-utilization", "0.99", "--max-model-len", "16384"], "recipe_ephemeral_storage_size": 30, "recipe_node_boot_volume_size_in_gbs": 300, "recipe_shared_memory_volume_size_limit_in_mb": 1000, "recipe_startup_probe_params": {"failure_threshold": 30, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 10, "period_seconds": 2, "success_threshold": 1, "timeout_seconds": 1}, "recipe_liveness_probe_params": {"failure_threshold": 3, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 65, "period_seconds": 600, "success_threshold": 1, "timeout_seconds": 1}, "recipe_pod_autoscaling_params": {"min_replicas": 1, "max_replicas": 50}}, {"pre_filled_sample_name": "Update MIG Configuration by Node Name", "recipe_mode": "update", "deployment_name": "all-1g10gb", "recipe_node_name": "10.0.10.138", "shared_node_pool_mig_config": "all-1g.10gb"}, {"pre_filled_sample_name": "Update MIG Configuration by Node Pool Name", "recipe_mode": "update", "deployment_name": "all-2g-20gb", "recipe_node_pool_name": "h100migpool", "shared_node_pool_mig_config": "all-2g.20gb"}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Multi-Node Inference", "blueprint_short_description": "Scale large language model inference across multiple GPU nodes using tensor and pipeline parallelism", "blueprint_long_description": "Multi-node inference enables deploying very large language models that cannot fit within the GPU memory of a single node by distributing the workload across multiple computing nodes. This approach combines tensor parallelism (splitting operations across GPUs within a node) and pipeline parallelism (distributing sequential stages across nodes) to efficiently utilize available hardware resources.\n\nThis blueprint is essential when serving models like Llama-3.3-70B-Instruct that require approximately 150GB of GPU memory, exceeding the capacity of single-node configurations. The system uses vLLM and Ray with LeaderWorkerSet (LWS) to manage distributed state across nodes, creating a cluster with one head node and multiple worker nodes.\n\nThe multi-node approach significantly reduces processing time and improves throughput for both real-time and batch predictions. It requires careful planning to determine the appropriate node shapes and GPU requirements based on model size, precision, and available compute shapes. The system supports shared node pools and optional RDMA connectivity for enhanced performance.\n\nKey benefits include the ability to serve models that exceed single-node memory limits, improved inference throughput through parallel processing, and efficient resource utilization across distributed GPU infrastructure.", "pre_filled_samples": [{"pre_filled_sample_name": "Multi-Node Inference on VM.GPU.A10 Cluster", "recipe_id": "vllm_multinode_inference", "recipe_mode": "service", "deployment_name": "multinode_inference", "recipe_node_shape": "VM.GPU.A10.2", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:ray2430_vllmv083", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]}], "recipe_replica_count": 1, "recipe_nvidia_gpu_count": 2, "recipe_ephemeral_storage_size": 150, "recipe_shared_memory_volume_size_limit_in_mb": 10000, "recipe_container_port": "8000", "recipe_use_shared_node_pool": true, "multinode_num_nodes_to_use_from_shared_pool": 2, "recipe_container_command_args": ["--port", "8000", "--model", "/models", "--tensor-parallel-size", "2", "--pipeline-parallel-size", "2", "--gpu-memory-utilization", "0.90", "--distributed-executor-backend", "ray"], "recipe_readiness_probe_params": {"endpoint_path": "/health", "port": 8000, "initial_delay_seconds": 20, "period_seconds": 10}}, {"pre_filled_sample_name": "Multi-Node Inference on BM.GPU.A10 Cluster", "recipe_id": "vllm_multinode_inference", "recipe_mode": "service", "deployment_name": "multinode_inference", "recipe_node_shape": "BM.GPU.A10.4", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:ray2430_vllmv083", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]}], "recipe_replica_count": 1, "recipe_nvidia_gpu_count": 4, "recipe_ephemeral_storage_size": 150, "recipe_shared_memory_volume_size_limit_in_mb": 10000, "recipe_container_port": "8000", "recipe_use_shared_node_pool": true, "multinode_num_nodes_to_use_from_shared_pool": 2, "recipe_container_command_args": ["--port", "8000", "--model", "/models", "--tensor-parallel-size", "4", "--pipeline-parallel-size", "2", "--gpu-memory-utilization", "0.90", "--distributed-executor-backend", "ray"], "recipe_readiness_probe_params": {"endpoint_path": "/health", "port": 8000, "initial_delay_seconds": 20, "period_seconds": 10}}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Install OCI AI Blueprints onto an Existing OKE Cluster", "blueprint_short_description": "Deploy OCI AI Blueprints on your existing OKE cluster without creating new infrastructure", "blueprint_long_description": "This guide helps you install and use **OCI AI Blueprints** on an existing OKE cluster that was created outside of blueprints and already has workflows running on it. Rather than installing blueprints onto a new cluster, you can leverage an existing cluster with node pools and tools already installed.\n\nThe installation process involves ensuring you have the correct IAM policies in place, retrieving existing cluster OKE and VCN information from the console, deploying the OCI AI Blueprints application onto the existing cluster, and optionally adding existing nodes to be used by blueprints. You can then deploy sample recipes to test functionality.\n\nKey considerations include managing existing tooling like Prometheus, Grafana, or the GPU operator that may already be installed on your cluster. The blueprint installation process can detect and work around these existing components. Additionally, if you have the nvidia-gpu-operator installed and plan to use Multi-Instance GPUs with H100 nodes, special configuration steps are available.\n\nThis approach allows you to:\n\n- Leverage existing cluster resources and configurations\n\n- Add blueprints capabilities without disrupting current workloads\n\n- Utilize existing node pools for blueprint deployments\n\n- Maintain compatibility with pre-installed cluster tools", "pre_filled_samples": [{"pre_filled_sample_name": "Add Existing Node to Control Plane", "recipe_mode": "update", "deployment_name": "startupaddnode", "recipe_node_name": "10.0.10.164", "recipe_node_labels": {"corrino": "a10pool", "corrino/pool-shared-any": "true"}}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Llama Stack on OCI", "blueprint_short_description": "Pre-packaged GenAI runtime — vLLM + ChromaDB + Postgres (optional Jaeger) ready for one-click deployment", "blueprint_long_description": "Deploy Llama Stack on OCI via OCI AI Blueprints. For more information on Llama Stack: https://github.com/meta-llama/llama-stack\n\nWe are using Postgres for the backend store, chromaDB for the vector database, Jaeger for tracing and vLLM for inference serving.", "pre_filled_samples": [{"pre_filled_sample_name": "Llama 3.1 8B Model with vLLM", "deployment_group": {"name": "group", "deployments": [{"name": "postgres", "recipe": {"recipe_id": "postgres", "deployment_name": "postgres", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/library/postgres:latest", "recipe_container_port": "5432", "recipe_host_port": "5432", "recipe_container_env": [{"key": "POSTGRES_USER", "value": "llamastack"}, {"key": "POSTGRES_PASSWORD", "value": "llamastack"}, {"key": "POSTGRES_DB", "value": "llamastack"}], "recipe_replica_count": 1}, "exports": ["internal_dns_name"]}, {"name": "chroma", "recipe": {"recipe_id": "chromadb", "deployment_name": "chroma", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/chromadb/chroma:latest", "recipe_container_port": "8000", "recipe_host_port": "8000", "recipe_container_env": [{"key": "IS_PERSISTENT", "value": "TRUE"}, {"key": "ANONYMIZED_TELEMETRY", "value": "FALSE"}], "recipe_replica_count": 1, "output_object_storage": [{"bucket_name": "chromadb", "mount_location": "/chroma/chroma", "volume_size_in_gbs": 500}]}, "exports": ["internal_dns_name"]}, {"name": "vllm", "recipe": {"recipe_id": "llm_inference_nvidia", "deployment_name": "vllm", "recipe_mode": "service", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:vllmv0.6.6.pos1", "recipe_node_shape": "VM.GPU.A10.2", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]}], "recipe_container_env": [{"key": "tensor_parallel_size", "value": "2"}, {"key": "model_name", "value": "NousResearch/Meta-Llama-3.1-8B-Instruct"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B-Instruct"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "$(Model_Path)", "--tensor-parallel-size", "$(tensor_parallel_size)"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 200}, "exports": ["internal_dns_name"]}, {"name": "jaeger", "recipe": {"recipe_id": "jaeger", "deployment_name": "jaeger", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/jaegertracing/jaeger:latest", "recipe_container_port": "16686", "recipe_additional_ingress_ports": [{"name": "jaeger", "port": 4318, "path": "/jaeger"}], "recipe_replica_count": 1}, "exports": ["internal_dns_name"]}, {"name": "llamastack_app", "recipe": {"recipe_id": "llamastack_app", "deployment_name": "llamastack_app", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/llamastack/distribution-postgres-demo:latest", "recipe_container_port": "8321", "recipe_container_env": [{"key": "INFERENCE_MODEL", "value": "/models/NousResearch/Meta-Llama-3.1-8B-Instruct"}, {"key": "VLLM_URL", "value": "http://${vllm.internal_dns_name}/v1"}, {"key": "ENABLE_CHROMADB", "value": "1"}, {"key": "CHROMADB_URL", "value": "http://${chroma.internal_dns_name}:8000"}, {"key": "POSTGRES_HOST", "value": "${postgres.internal_dns_name}"}, {"key": "POSTGRES_PORT", "value": "5432"}, {"key": "POSTGRES_DB", "value": "llamastack"}, {"key": "POSTGRES_USER", "value": "llamastack"}, {"key": "POSTGRES_PASSWORD", "value": "llamastack"}, {"key": "TELEMETRY_SINKS", "value": "console,otel_trace"}, {"key": "OTEL_TRACE_ENDPOINT", "value": "http://${jaeger.internal_dns_name}/jaeger/v1/traces"}], "output_object_storage": [{"bucket_name": "llamastack", "mount_location": "/root/.llama", "volume_size_in_gbs": 100}], "recipe_replica_count": 1}, "depends_on": ["postgres", "chroma", "vllm", "jaeger"]}]}}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Model Storage", "blueprint_short_description": "Download and store models from HuggingFace to OCI Object Storage for efficient blueprint deployment", "blueprint_long_description": "Model storage is a critical component for AI/ML workloads, providing efficient access to large language models and other AI assets. OCI AI Blueprints supports storing models in OCI Object Storage, which offers faster loading times and better resource management compared to downloading models directly from HuggingFace during container startup.\n\nThis blueprint provides automated workflows to download models from HuggingFace (both open and gated models) and store them in OCI Object Storage buckets. Once stored, these models can be efficiently accessed by inference blueprints through pre-authenticated requests (PARs) or direct bucket access, significantly reducing deployment times and improving reliability.\n\nThe system supports both open-source models that require no authentication and closed/gated models that require HuggingFace tokens for access. Models are downloaded using optimized parallel workers and stored with appropriate volume sizing to accommodate large model files.", "pre_filled_samples": [{"pre_filled_sample_name": "Download Closed HuggingFace Model to Object Storage", "recipe_id": "example", "recipe_mode": "job", "deployment_name": "model_to_object", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:hf_downloader_v1", "recipe_container_command_args": ["meta-llama/Llama-3.2-90B-Vision-Instruct", "--local-dir", "/models", "--max-workers", "4", "--token", "<hf_token>"], "recipe_container_port": "5678", "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_node_pool_size": 1, "recipe_flex_shape_ocpu_count": 4, "recipe_flex_shape_memory_size_in_gbs": 64, "recipe_node_boot_volume_size_in_gbs": 500, "recipe_ephemeral_storage_size": 450, "output_object_storage": [{"bucket_name": "llama3290Bvisioninstruct", "mount_location": "/models", "volume_size_in_gbs": 450}]}, {"pre_filled_sample_name": "Download Open HuggingFace Model to Object Storage", "recipe_id": "example", "recipe_mode": "job", "deployment_name": "model_to_object", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:hf_downloader_v1", "recipe_container_command_args": ["NousResearch/Meta-Llama-3.1-405B-FP8", "--local-dir", "/models", "--max-workers", "16"], "recipe_container_port": "5678", "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_node_pool_size": 1, "recipe_flex_shape_ocpu_count": 16, "recipe_flex_shape_memory_size_in_gbs": 256, "recipe_node_boot_volume_size_in_gbs": 1000, "recipe_ephemeral_storage_size": 900, "output_object_storage": [{"bucket_name": "nousllama31405bfp8", "mount_location": "/models", "volume_size_in_gbs": 800}]}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Using RDMA Enabled Node Pools", "blueprint_short_description": "Enable high-performance inter-node communication using Remote Direct Memory Access for large-scale AI workloads", "blueprint_long_description": "Remote Direct Memory Access (RDMA) is a protocol that enables one node to read from or write to the memory of another node without involving either machine's CPU or operating system, enabling true zero-copy data transfers and dramatically reducing latency and CPU overhead. In large-scale AI workloads such as multi-node training with AllReduce or disaggregated LLM inference, RDMA can yield tremendous performance gains by significantly reducing communication and copy overhead between nodes.\n\nOCI AI Blueprints uses OCI cluster networks with instance pools to provision RDMA-enabled node pools, supporting high-performance compute shapes including BM.GPU.H100.8, BM.GPU.H200.8, and BM.GPU.B4.8. The system requires custom node images with proper drivers and libraries for RDMA connectivity, which must be imported from the oci-hpc-oke quickstart repository.\n\nRDMA-enabled deployments are particularly valuable for distributing very large language models (like Llama-3.1-405B-Instruct) that exceed single-node GPU memory capacity, requiring distributed inference across multiple nodes with high-bandwidth, low-latency communication. The technology enables efficient tensor and pipeline parallelism by eliminating traditional network communication bottlenecks.\n\nThe implementation supports both creating new RDMA-enabled shared node pools and integrating OCI AI Blueprints with existing RDMA-enabled clusters, providing flexibility for various deployment scenarios and infrastructure configurations.", "pre_filled_samples": [{"pre_filled_sample_name": "RDMA-Enabled H100 Shared Node Pool", "deployment_name": "H100_rdma_pool", "recipe_mode": "shared_node_pool", "shared_node_pool_size": 2, "shared_node_pool_shape": "BM.GPU.H100.8", "shared_node_pool_boot_volume_size_in_gbs": 1000, "recipe_availability_domain": "TrcQ:EU-FRANKFURT-1-AD-3", "recipe_node_image_ocid": "ocid1.image.oc1.eu-frankfurt-1.aaaaaaaakhpy5kt3p6gjmeqbasnndemp6aetlnbkm57hohrkgksuh4476llq", "multinode_rdma_enabled_in_shared_pool": true}, {"pre_filled_sample_name": "RDMA Distributed Inference (405B Model)", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "405b", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:ray2430_vllmv083", "recipe_node_shape": "BM.GPU.H100.8", "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 8, "recipe_use_shared_node_pool": true, "multinode_rdma_enabled_in_shared_pool": true, "multinode_num_nodes_to_use_from_shared_pool": 2, "input_object_storage": [{"par": "https://iduyx1qnmway.objectstorage.eu-frankfurt-1.oci.customer-oci.com/p/7N2O5JFirNX_CG70t-HPILzHvlTMP4FC9f_eauJVECosqNafIYxwcDwhItQHvaDK/n/iduyx1qnmway/b/llama31405binstruct/o/", "mount_location": "/models", "volume_size_in_gbs": 500}], "recipe_container_env": [{"key": "NCCL_DEBUG", "value": "INFO"}, {"key": "NCCL_DEBUG_SUBSYS", "value": "INIT,NET,ENV"}], "recipe_readiness_probe_params": {"endpoint_path": "/health", "port": 8000, "initial_delay_seconds": 20, "period_seconds": 10}, "recipe_container_command_args": ["--port", "8000", "--model", "/models", "--tensor-parallel-size", "8", "--gpu-memory-utilization", "0.90", "--pipeline-parallel-size", "2", "--distributed-executor-backend", "ray"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 10000}, {"pre_filled_sample_name": "Update Nodes for RDMA Support", "recipe_mode": "update", "deployment_name": "startupaddnode1", "recipe_node_name": "10.0.10.164", "recipe_node_labels": {"corrino": "h100pool", "corrino/pool-shared-any": "true", "corrino/rdma": "true"}}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Whisper Transcription API", "blueprint_short_description": "This blueprint provides a complete solution for running **audio/video transcription**, **speaker diarization**, and **summarization** via a RESTful API. It integrates [Faster-Whisper](https://github.com/guillaumekln/faster-whisper) for efficient transcription, [pyannote.audio](https://github.com/pyannote/pyannote-audio) for diarization, and Hugging Face instruction-tuned LLMs (e.g., Mistral-7B) for summarization. It supports multi-GPU acceleration, real-time streaming logs, and JSON/text output formats.", "blueprint_long_description": "---", "pre_filled_samples": [{"pre_filled_sample_name": "Deploy Whisper transcription on A10 GPU for real-time speech-to-text", "recipe_id": "whisper  transcription", "recipe_mode": "service", "deployment_name": "whisper-transcription-a10", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:whisper_transcription_v8", "recipe_node_shape": "VM.GPU.A10.2", "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}, {"pre_filled_sample_name": "Deploy Whisper transcription on A100 GPU for high-speed processing", "recipe_id": "whisper  transcription", "recipe_mode": "service", "deployment_name": "whisper-transcription-a100", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:whisper_transcription_v8", "recipe_node_shape": "BM.GPU.A100.8", "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 8, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}, {"pre_filled_sample_name": "Deploy Whisper transcription on H100 GPU for next-gen AI workloads", "recipe_id": "whisper  transcription", "recipe_mode": "service", "deployment_name": "whisper-transcription-h100", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:whisper_transcription_v8", "recipe_node_shape": "BM.GPU.H100.8", "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 8, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Deployment Groups", "blueprint_short_description": "Connected multi-container deployments in a single blueprint", "blueprint_long_description": "Deployment Groups let you spin up several deployments — each derived from its own blueprint — in a single `POST /deployment` request and treat them as one cohesive application. OCI AI Blueprints automatically sequences those member deployments according to the depends_on relationships you declare, publishes each deployment’s outputs (such as service URLs or internal dns name) for easy discovery, and then injects those outputs wherever you reference the placeholder `${deployment_name.export_key}` inside downstream blueprints. What once required a series of separate API calls stitched together with hard-coded endpoints can now be expressed declaratively in one step, with OCI AI Blueprints resolving every cross-service connection at runtime.", "pre_filled_samples": [{"pre_filled_sample_name": "Deployment Groups Showcase: Llama Stack", "deployment_group": {"name": "group", "deployments": [{"name": "postgres", "recipe": {"recipe_id": "postgres", "deployment_name": "postgres", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/library/postgres:latest", "recipe_container_port": "5432", "recipe_host_port": "5432", "recipe_container_env": [{"key": "POSTGRES_USER", "value": "llamastack"}, {"key": "POSTGRES_PASSWORD", "value": "llamastack"}, {"key": "POSTGRES_DB", "value": "llamastack"}], "recipe_replica_count": 1}, "exports": ["internal_dns_name"]}, {"name": "chroma", "recipe": {"recipe_id": "chromadb", "deployment_name": "chroma", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/chromadb/chroma:latest", "recipe_container_port": "8000", "recipe_host_port": "8000", "recipe_container_env": [{"key": "IS_PERSISTENT", "value": "TRUE"}, {"key": "ANONYMIZED_TELEMETRY", "value": "FALSE"}], "recipe_replica_count": 1, "output_object_storage": [{"bucket_name": "chromadb", "mount_location": "/chroma/chroma", "volume_size_in_gbs": 500}]}, "exports": ["internal_dns_name"]}, {"name": "vllm", "recipe": {"recipe_id": "llm_inference_nvidia", "deployment_name": "vllm", "recipe_mode": "service", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:vllmv0.6.6.pos1", "recipe_node_shape": "VM.GPU.A10.2", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]}], "recipe_container_env": [{"key": "tensor_parallel_size", "value": "2"}, {"key": "model_name", "value": "NousResearch/Meta-Llama-3.1-8B-Instruct"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B-Instruct"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "$(Model_Path)", "--tensor-parallel-size", "$(tensor_parallel_size)"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 200}, "exports": ["internal_dns_name"]}, {"name": "jaeger", "recipe": {"recipe_id": "jaeger", "deployment_name": "jaeger", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/jaegertracing/jaeger:latest", "recipe_container_port": "16686", "recipe_additional_ingress_ports": [{"name": "jaeger", "port": 4318, "path": "/jaeger"}], "recipe_replica_count": 1}, "exports": ["internal_dns_name"]}, {"name": "llamastack_app", "recipe": {"recipe_id": "llamastack_app", "deployment_name": "llamastack_app", "recipe_mode": "service", "recipe_node_pool_size": 1, "recipe_node_shape": "VM.Standard.E4.Flex", "recipe_flex_shape_ocpu_count": 2, "recipe_flex_shape_memory_size_in_gbs": 16, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_image_uri": "docker.io/llamastack/distribution-postgres-demo:latest", "recipe_container_port": "8321", "recipe_container_env": [{"key": "INFERENCE_MODEL", "value": "/models/NousResearch/Meta-Llama-3.1-8B-Instruct"}, {"key": "VLLM_URL", "value": "http://${vllm.internal_dns_name}/v1"}, {"key": "ENABLE_CHROMADB", "value": "1"}, {"key": "CHROMADB_URL", "value": "http://${chroma.internal_dns_name}:8000"}, {"key": "POSTGRES_HOST", "value": "${postgres.internal_dns_name}"}, {"key": "POSTGRES_PORT", "value": "5432"}, {"key": "POSTGRES_DB", "value": "llamastack"}, {"key": "POSTGRES_USER", "value": "llamastack"}, {"key": "POSTGRES_PASSWORD", "value": "llamastack"}, {"key": "TELEMETRY_SINKS", "value": "console,otel_trace"}, {"key": "OTEL_TRACE_ENDPOINT", "value": "http://${jaeger.internal_dns_name}/jaeger/v1/traces"}], "output_object_storage": [{"bucket_name": "llamastack", "mount_location": "/root/.llama", "volume_size_in_gbs": 100}], "recipe_replica_count": 1}, "depends_on": ["postgres", "chroma", "vllm", "jaeger"]}]}}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Shared Node Pools", "blueprint_short_description": "Create persistent node pools for efficient blueprint deployment without infrastructure recycling", "blueprint_long_description": "Shared node pools enable you to launch infrastructure independent of individual blueprints, allowing multiple blueprints to deploy and undeploy on the same underlying infrastructure without the overhead of spinning up new node pools for each deployment. This approach eliminates the time-consuming process of infrastructure provisioning and teardown, particularly beneficial for bare metal shapes that require longer recycle times.\n\nWhen you deploy a standard blueprint, OCI AI Blueprints creates a separate node pool for each blueprint and destroys it upon undeployment. Shared node pools solve this inefficiency by providing persistent infrastructure that can host multiple blueprints simultaneously or sequentially. This is especially valuable when you want to deploy multiple blueprints on the same hardware (e.g., two blueprints each using 2 GPUs on a 4-GPU shape) or need rapid deployment cycles.\n\nThe system supports both selector-based and non-selector deployment strategies. With selectors, you can use naming conventions to ensure specific blueprints land on designated shared node pools, providing precise control over resource allocation. Without selectors, blueprints will deploy to any available shared node pool matching the required shape.\n\nShared node pools are compatible with any blueprint and support all OCI compute shapes, with special considerations for bare metal configurations that require boot volume size specifications.\n\n**Note**: The list of shapes below are supported by Blueprints, but not yet supported by OKE, requiring blueprints to treat them as self-managed nodes. These require:\n\n1. Specifying the Availability Domain of the instance type\n\n2. Specifying the custom image OCID to use for the node\n\nAdditional required fields:\n\n```json\n\n\"recipe_availability_domain\": \"<Availability Domain>\",\n\n\"recipe_node_image_ocid\": \"<ocid>\"\n\n```\n\nSee [this recipe](./shared_node_pool_B200_BM.json) as an example for these parameters.\n\n[This document section](../using_rdma_enabled_node_pools/README.md#import-a-custom-image) describes now to import a custom image and provides links to import custom images for various shapes.", "pre_filled_samples": [{"pre_filled_sample_name": "Shared Node Pool for BM.GPU.A10", "deployment_name": "BM.GPU.A10.4 shared pool", "recipe_mode": "shared_node_pool", "shared_node_pool_size": 2, "shared_node_pool_shape": "BM.GPU.A10.4", "shared_node_pool_boot_volume_size_in_gbs": 500}, {"pre_filled_sample_name": "Shared Node Pool for VM.GPU.A10", "deployment_name": "VM.GPU.A10.2 shared pool", "recipe_mode": "shared_node_pool", "shared_node_pool_size": 2, "shared_node_pool_shape": "VM.GPU.A10.2", "shared_node_pool_boot_volume_size_in_gbs": 500}, {"pre_filled_sample_name": "vLLM Inference on Shared Pool", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "vLLM Inference Deployment", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "BM.GPU.A10.4", "input_object_storage": [{"par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/", "mount_location": "/models", "volume_size_in_gbs": 500, "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]}], "recipe_container_env": [{"key": "tensor_parallel_size", "value": "2"}, {"key": "model_name", "value": "NousResearch/Meta-Llama-3.1-8B-Instruct"}, {"key": "Model_Path", "value": "/models/NousResearch/Meta-Llama-3.1-8B-Instruct"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 2, "recipe_use_shared_node_pool": true, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_container_command_args": ["--model", "$(Model_Path)", "--tensor-parallel-size", "$(tensor_parallel_size)"], "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Startup, Liveness, and Readiness Probes", "blueprint_short_description": "Configure application health monitoring and startup validation for reliable service deployment", "blueprint_long_description": "Startup, Liveness, and Readiness Probes are essential Kubernetes tools that ensure your applications are truly ready to serve traffic and remain healthy throughout their lifecycle. These probes are particularly critical for LLM inference services that require time to load model weights before becoming ready to serve requests.\n\nThis blueprint demonstrates how to configure these probes with any OCI AI Blueprint deployment to improve service reliability and prevent traffic routing to unhealthy containers. The probes can be applied to any blueprint type - inference, training, or custom workloads - providing consistent health monitoring across your AI infrastructure.", "pre_filled_samples": [{"pre_filled_sample_name": "vLLM Autoscaling with Health Probes", "recipe_id": "llm_inference_nvidia", "recipe_mode": "service", "deployment_name": "autoscale_with_fss", "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1", "recipe_node_shape": "VM.GPU.A10.2", "recipe_container_env": [{"key": "tensor_parallel_size", "value": "1"}, {"key": "Model_Path", "value": "/models/models/meta-llama/Llama-3.2-1B-Instruct"}], "recipe_replica_count": 1, "recipe_container_port": "8000", "recipe_nvidia_gpu_count": 1, "recipe_container_command_args": ["--model", "$(Model_Path)", "--tensor-parallel-size", "$(tensor_parallel_size)", "--gpu-memory-utilization", "0.99", "--max-model-len", "1024"], "recipe_ephemeral_storage_size": 200, "recipe_node_boot_volume_size_in_gbs": 300, "recipe_node_pool_size": 1, "recipe_shared_memory_volume_size_limit_in_mb": 200, "recipe_startup_probe_params": {"failure_threshold": 30, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 10, "period_seconds": 2, "success_threshold": 1, "timeout_seconds": 1}, "recipe_liveness_probe_params": {"failure_threshold": 3, "endpoint_path": "/health", "port": 8000, "scheme": "HTTP", "initial_delay_seconds": 65, "period_seconds": 600, "success_threshold": 1, "timeout_seconds": 1}, "recipe_pod_autoscaling_params": {"min_replicas": 1, "max_replicas": 4}, "recipe_node_autoscaling_params": {"min_nodes": 1, "max_nodes": 2}, "input_file_system": [{"file_system_ocid": "ocid1.filesystem.oc1.iad.aaaaaaaaaaklirslnfqwillqojxwiotjmfsc2ylefuzqaaaa", "mount_target_ocid": "ocid1.mounttarget.oc1.iad.aaaaacvipp3o7rlwnfqwillqojxwiotjmfsc2ylefuzqaaaa", "mount_location": "/models", "volume_size_in_gbs": 50}]}]}, {"blueprint_type": "oci_blueprint", "blueprint_category": "workload_blueprint", "blueprint_title": "Teams", "blueprint_short_description": "Enforce resource quotas and fair sharing between teams using Kueue job queuing for efficient cluster utilization", "blueprint_long_description": "Teams in OCI AI Blueprints enables administrators to enforce resource quotas and fair sharing between different organizational units, ensuring efficient allocation of GPU and CPU resources across multiple teams within a shared cluster. The system leverages Kueue, a Kubernetes job queuing system, to manage AI/ML workloads with workload queuing, prioritization, and resource-aware scheduling.\n\nEach team functions as a logical grouping backed by a Kueue ClusterQueue and LocalQueue, with configurable nominal quotas (guaranteed resources), borrowing limits (extra resources when available), and lending limits (idle resources offered to other teams). This approach enables fair sharing, dynamic resource allocation, and improved utilization across workloads while maintaining strict resource boundaries.\n\nThe team system supports multi-tenant clusters where business units, research groups, or customers can be isolated while still sharing idle GPU/CPU capacity. Jobs are admitted based on available quotas and resource policies, with priority thresholds determining which teams can exceed their nominal quotas when extra resources are available.\n\nTeams are particularly valuable for capacity planning, expressing organizational-level GPU budgets in code, and tracking consumption across different groups. The system automatically handles resource borrowing and lending through a shared cohort, ensuring that resources never sit idle while respecting team boundaries and priorities.", "pre_filled_samples": [{"pre_filled_sample_name": "Create Team with Resource Quotas", "recipe_mode": "team", "deployment_name": "create_team", "team": {"team_name": "randomteam", "priority_threshold": 100, "quotas": [{"shape_name": "BM.GPU.H100.8", "cpu_nominal_quota": "10", "cpu_borrowing_limit": "4", "cpu_lending_limit": "4", "mem_nominal_quota": "10", "mem_borrowing_limit": "4", "mem_lending_limit": "4", "gpu_nominal_quota": "10", "gpu_borrowing_limit": "4", "gpu_lending_limit": "4"}, {"shape_name": "VM.GPU.A10.2", "cpu_nominal_quota": "10", "cpu_borrowing_limit": "4", "cpu_lending_limit": "4", "mem_nominal_quota": "10", "mem_borrowing_limit": "4", "mem_lending_limit": "4", "gpu_nominal_quota": "10", "gpu_borrowing_limit": "4", "gpu_lending_limit": "4"}]}}, {"pre_filled_sample_name": "Create Job with Team Assignment", "recipe_id": "healthcheck", "recipe_mode": "job", "deployment_name": "create_job_with_team", "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:healthcheck_v0.3", "recipe_node_shape": "VM.GPU.A10.2", "recipe_use_shared_node_pool": true, "recipe_team_info": {"team_name": "randomteam"}, "output_object_storage": [{"bucket_name": "healthcheck2", "mount_location": "/healthcheck_results", "volume_size_in_gbs": 20}], "recipe_container_command_args": ["--dtype", "float16", "--output_dir", "/healthcheck_results", "--expected_gpus", "A10:2,A100:0,H100:0"], "recipe_replica_count": 1, "recipe_nvidia_gpu_count": 2, "recipe_node_pool_size": 1, "recipe_node_boot_volume_size_in_gbs": 200, "recipe_ephemeral_storage_size": 100, "recipe_shared_memory_volume_size_limit_in_mb": 1000, "recipe_container_cpu_count": 4, "recipe_container_memory_size": 20}]}]