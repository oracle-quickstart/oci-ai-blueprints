{
    "recipe_id": "llm_inference_nvidia",
    "recipe_mode": "service",
    "deployment_name": "vllm-from-hf",
    "recipe_image_uri": "docker.io/vllm/vllm-openai:v0.9.1",
    "recipe_node_shape": "VM.GPU.A10.1",
    "recipe_replica_count": 1,
    "recipe_container_port": "8000",
    "recipe_prometheus_enabled": true,
    "recipe_nvidia_gpu_count": 1,
    "recipe_node_pool_size": 1,
    "recipe_node_boot_volume_size_in_gbs": 200,
    "recipe_environment_secrets": [
      {
          "envvar_name": "HF_TOKEN",
          "secret_name": "hf-secret",
          "secret_key": "hf-token"
      }
    ],
    "recipe_container_command_args": [
        "--model",
        "meta-llama/Llama-3.2-1B-Instruct",
        "--max-model-len",
        "4096",
        "--gpu-memory-utilization",
        "0.95"
    ],
    "recipe_ephemeral_storage_size": 100,
    "recipe_shared_memory_volume_size_limit_in_mb": 16384,
    "recipe_readiness_probe_params": {
        "endpoint_path": "/health",
        "port": 8000,
        "initial_delay_seconds": 20,
        "period_seconds": 10
    }
}
  