{
  "recipe_id": "vllm_multinode_inference",
  "recipe_mode": "service",
  "deployment_name": "multinode_inference",
  "recipe_node_shape": "BM.GPU.A10.4",
  "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:ray2430_vllmv083",
  "input_object_storage": [
    {
      "par": "https://objectstorage.us-ashburn-1.oraclecloud.com/p/IFknABDAjiiF5LATogUbRCcVQ9KL6aFUC1j-P5NSeUcaB2lntXLaR935rxa-E-u1/n/iduyx1qnmway/b/corrino_hf_oss_models/o/",
      "mount_location": "/models",
      "volume_size_in_gbs": 500,
      "include": ["NousResearch/Meta-Llama-3.1-8B-Instruct"]
    }
  ],
  "recipe_replica_count": 1,
  "recipe_nvidia_gpu_count": 4,
  "recipe_ephemeral_storage_size": 150,
  "recipe_shared_memory_volume_size_limit_in_mb": 10000,
  "recipe_container_port": "8000",
  "recipe_use_shared_node_pool": true,
  "multinode_num_nodes_to_use_from_shared_pool": 2,
  "recipe_container_command_args": [
    "--port",
    "8000",
    "--model",
    "/models",
    "--tensor-parallel-size",
    "4",
    "--pipeline-parallel-size",
    "2",
    "--gpu-memory-utilization",
    "0.90",
    "--distributed-executor-backend",
    "ray"
  ],
  "recipe_readiness_probe_params": {
      "endpoint_path": "/health",
      "port": 8000,
      "initial_delay_seconds": 20,
      "period_seconds": 10
  }
}

